{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0nq1jznXcTa"
      },
      "outputs": [],
      "source": [
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3OO9RM0Exmw",
        "outputId": "22561dd6-7371-4f54-f1b8-ddf02c6cf1e7"
      },
      "outputs": [],
      "source": [
        "#@title Importing required packages\n",
        "\n",
        "%pip install pdfminer.six\n",
        "%pip install docx2txt\n",
        "%pip install resume_parser\n",
        "\n",
        "\n",
        "from pdfminer.high_level import extract_text\n",
        "import pandas as pd\n",
        "import docx2txt\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poFQAdOTF1SZ",
        "outputId": "8d4bcb3b-ac90-4a2a-8215-7450439b815d"
      },
      "outputs": [],
      "source": [
        "#@title Downloading required modules\n",
        "\n",
        "\n",
        "# Downloading required\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PTaPKMEGBxv"
      },
      "outputs": [],
      "source": [
        "#@title Extracting text from PDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzFUh9XIGKzD"
      },
      "outputs": [],
      "source": [
        "#@title Extracting text from Doc\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    txt = docx2txt.process(docx_path)\n",
        "    if txt:\n",
        "        return txt.replace('\\t', ' ')\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsUh2Xy_GfUX"
      },
      "outputs": [],
      "source": [
        "#@title Extracting name using nltk\n",
        "\n",
        "def extract_names(txt):\n",
        "    person_names = []\n",
        "\n",
        "    for sent in nltk.sent_tokenize(txt):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
        "                person_names.append(\n",
        "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
        "                )\n",
        "\n",
        "    return person_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3xoGkObHJYF"
      },
      "outputs": [],
      "source": [
        "#@title Extracting name using Spacy\n",
        "\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "def person(txt):\n",
        "    text1= NER(txt)\n",
        "    lst = []\n",
        "    for word in text1.ents:\n",
        "        if word.label_ == \"PERSON\":\n",
        "            lst.append(word.text)\n",
        "    return lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoXXvPJzGuwe"
      },
      "outputs": [],
      "source": [
        "#@title Extracting Phone Number\n",
        "\n",
        "def mobile_number(text):\n",
        "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
        "\n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "        if len(number) > 10:\n",
        "            return '+' + number\n",
        "        else:\n",
        "            return number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVO24xBSHlks"
      },
      "outputs": [],
      "source": [
        "#@title Extracting Email\n",
        "\n",
        "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
        "def extract_emails(resume_text):\n",
        "    return re.findall(EMAIL_REG, resume_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_UHIotBH7HJ"
      },
      "outputs": [],
      "source": [
        "#@title Extracting Education and School using nltk\n",
        "\n",
        "\n",
        "RESERVED_WORDS = [\n",
        "    'school',\n",
        "    'college',\n",
        "    'univers',\n",
        "    'academy',\n",
        "    'faculty',\n",
        "    'institute',\n",
        "    'faculdades',\n",
        "    'Schola',\n",
        "    'schule',\n",
        "    'lise',\n",
        "    'lyceum',\n",
        "    'lycee',\n",
        "    'polytechnic',\n",
        "    'kolej',\n",
        "    'ünivers',\n",
        "    'okul',\n",
        "]\n",
        "\n",
        "\n",
        "def extract_education(input_text):\n",
        "    organizations = []\n",
        "\n",
        "    # first get all the organization names using nltk\n",
        "    for sent in nltk.sent_tokenize(input_text):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
        "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
        "\n",
        "    # we search for each bigram and trigram for reserved words\n",
        "    # (college, university etc...)\n",
        "    education = set()\n",
        "    for org in organizations:\n",
        "        for word in RESERVED_WORDS:\n",
        "            if org.lower().find(word):\n",
        "                education.add(org)\n",
        "\n",
        "    return education\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zifZx5rjotIU"
      },
      "outputs": [],
      "source": [
        "#@title Extracting skills using dataset\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# noun_chunks = nlp.noun_chunks\n",
        "\n",
        "def extract_skills(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # removing stop words and implementing word tokenization\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "\n",
        "    # reading the csv file\n",
        "    data = pd.read_csv(\"/content/skills.csv\")\n",
        "\n",
        "    # extract values\n",
        "    skills = list(data.columns.values)\n",
        "\n",
        "    skillset = []\n",
        "\n",
        "    # check for one-grams (example: python)\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            skillset.append(token)\n",
        "\n",
        "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9MEWg4eIGs4"
      },
      "outputs": [],
      "source": [
        "#@title Extracting education and school using Spacy\n",
        "\n",
        "\n",
        "RESERVED_WORDS = [\n",
        "    'school',\n",
        "    'college',\n",
        "    'univers',\n",
        "    'academy',\n",
        "    'faculty',\n",
        "    'institute',\n",
        "    'faculdades',\n",
        "    'Schola',\n",
        "    'schule',\n",
        "    'lise',\n",
        "    'lyceum',\n",
        "    'lycee',\n",
        "    'polytechnic',\n",
        "    'kolej',\n",
        "    'ünivers',\n",
        "    'okul',\n",
        "]\n",
        "\n",
        "\n",
        "def extract_education_Spacy(input_text, skills):\n",
        "    organizations = []\n",
        "\n",
        "    text1= NER(input_text)\n",
        "\n",
        "    for word in text1.ents:\n",
        "        # if word.label_ == \"ORG\":\n",
        "        if word.label_ == \"ORG\" and word.text not in skills:\n",
        "            organizations.append(word.text)\n",
        "\n",
        "    education = set()\n",
        "    for org in organizations:\n",
        "        for word in RESERVED_WORDS:\n",
        "            if word.lower() in org.lower():\n",
        "                education.add(org)\n",
        "\n",
        "    return education\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMPr2jWqJMCn",
        "outputId": "57b36281-b579-4a68-f111-0a784e970214"
      },
      "outputs": [],
      "source": [
        "#@title Driver Code\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    file_path = '/content/Dishant_Thakkar_Resume.docx'  # replace with your file path\n",
        "    if os.path.splitext(file_path)[1] == '.pdf':\n",
        "        pdf_text = extract_text_from_pdf(file_path)\n",
        "\n",
        "        # Extracting name using nltk\n",
        "        names = extract_names(pdf_text)\n",
        "        print(\"NAME: \", names[0])\n",
        "\n",
        "        # Extracting name using spacy\n",
        "        nam = person(pdf_text)\n",
        "        print(\"Name : \", nam[0])\n",
        "\n",
        "        # Extracting phone number\n",
        "        phone_number = mobile_number(pdf_text)\n",
        "        print(\"Phone number : \", phone_number)\n",
        "\n",
        "        # Extracting email\n",
        "        email = extract_emails(pdf_text)\n",
        "        if email:\n",
        "            print(\"Email is : \", email)\n",
        "\n",
        "        # Extracting skills using\n",
        "        skills = extract_skills(pdf_text)\n",
        "        print(\"Skills : \", skills)\n",
        "\n",
        "        # Extracting education nltk\n",
        "        education = extract_education(pdf_text)\n",
        "        print(\"Education : \", list(education)[0])\n",
        "\n",
        "        # Extracting education spacy\n",
        "        education = extract_education_Spacy(pdf_text, skills)\n",
        "        print(\"Education : \", list(education)[0])\n",
        "\n",
        "\n",
        "    elif os.path.splitext(file_path)[1] == '.docx':\n",
        "        doc_text = extract_text_from_docx(file_path)\n",
        "\n",
        "        # Extracting name using nltk\n",
        "        names = extract_names(doc_text)\n",
        "        print(\"NAME : \", names[0])\n",
        "\n",
        "        # Extracting name using spacy\n",
        "        nam = person(doc_text)\n",
        "        print(\"Name : \", nam[0])\n",
        "\n",
        "        # Extracting phone number\n",
        "        phone_number = mobile_number(doc_text)\n",
        "        print(\"Phone number : \", phone_number)\n",
        "\n",
        "        # Extracting email\n",
        "        email = extract_emails(doc_text)\n",
        "        if email:\n",
        "            print(\"Email is : \", email)\n",
        "\n",
        "        # Extracting skills using\n",
        "        skills = extract_skills(doc_text)\n",
        "        print(\"Skills : \", skills)\n",
        "\n",
        "        # Extracting education nltk\n",
        "        education = extract_education(doc_text)\n",
        "        print(\"Education : \", list(education)[:2])\n",
        "\n",
        "\n",
        "        # Extracting education spacy\n",
        "        education = extract_education_Spacy(doc_text, skills)\n",
        "        print(\"Education : \", list(education)[:2])\n",
        "\n",
        "    else:\n",
        "        print(\"Unsupported file type\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJj9VsgHgQ_1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
